{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae8edb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: datasets==2.3.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: transformers==4.20.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (4.20.1)\n",
      "Requirement already satisfied: rouge-score in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (0.0.4)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (0.70.12.2)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (3.0.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (4.11.3)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (3.8.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (0.18.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (2.27.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (2022.5.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (4.63.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (1.3.5)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (8.0.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (0.3.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.3.0) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.20.1) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.20.1) (0.11.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.20.1) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.20.1) (3.6.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from rouge-score) (3.7)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from rouge-score) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.3.0) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from packaging->datasets==2.3.0) (3.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.3.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.3.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.3.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.3.0) (1.26.8)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.3.0) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.3.0) (20.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.3.0) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.3.0) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.3.0) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.3.0) (0.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->datasets==2.3.0) (3.7.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from nltk->rouge-score) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from nltk->rouge-score) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from pandas->datasets==2.3.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from pandas->datasets==2.3.0) (2021.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install \"datasets==2.3.0\" \"transformers==4.20.1\" \"rouge-score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140db514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to load all the Libraries and helper functions\n",
    "%run \"Utilities/libraries.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95690682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS TO THE SCRIPTS\n",
    "Bucket_Name=\"model-deploy-poc-new\"\n",
    "\n",
    "train_test_val_location_S3=\"train_test_val\"\n",
    "\n",
    "\n",
    "text_column=\"Article\"\n",
    "target_column=\"Summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1333f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names='google/pegasus-xsum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1c13dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=boto3.session.Session()\n",
    "role=sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea4abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = f\"s3://{Bucket_Name}/{train_test_val_location_S3}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01039b44",
   "metadata": {},
   "source": [
    "## 4. Invoke Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9461f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# Experiment with your models here. Change hyperparameters to optimize your results\n",
    "def model_invoke(model_name,train_test_val_location_S3):\n",
    "    output_dir_name=model_name.split(\"/\")[-1]\n",
    "    output_path = f\"s3://{Bucket_Name}/model/{output_dir_name}\"\n",
    "    # gets role for executing training job\n",
    "    role = sagemaker.get_execution_role()\n",
    "    hyperparameters = {\n",
    "        \"model-name\": model_name,\n",
    "        \"text-column\": text_column,\n",
    "        \"target-column\": target_column,\n",
    "        \"epoch\": 2,\n",
    "        'train-data-dir':input_path,\n",
    "        'log-dir':output_path+\"/Logs\",\n",
    "        'train-batch-size': 5,\n",
    "        'eval-batch-size': 2,\n",
    "     # more info here https://github.com/huggingface/transformers/tree/v4.17.0/examples/pytorch/summarization\n",
    "    }\n",
    "\n",
    "    metric_definitions = [\n",
    "        {\"Name\": \"training:loss\", \"Regex\": \"'loss': (.*?),\"},\n",
    "        {\"Name\": \"validation:loss\", \"Regex\": \"'eval_loss': (.*?),\"},\n",
    "        {\"Name\": \"validation:rouge1\", \"Regex\": \"'eval_rouge1': (.*?),\"},\n",
    "        {\"Name\": \"validation:rouge2\", \"Regex\": \"'eval_rouge2': (.*?),\"},\n",
    "        {\"Name\": \"validation:rougeL\", \"Regex\": \"'eval_rougeL': (.*?),\"},\n",
    "        {\"Name\": \"validation:rougeLsum\", \"Regex\": \"'eval_rougeLsum': (.*?),\"},\n",
    "        {\"Name\": \"validation:gen_len\", \"Regex\": \"'eval_gen_len': (.*?),\"},\n",
    "    ]\n",
    "\n",
    "    # git configuration to download our fine-tuning script\n",
    "    # git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.17.0'}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "    huggingface_estimator = HuggingFace(\n",
    "     entry_point='train.py',\n",
    "     source_dir='Utilities',\n",
    "     instance_type='ml.p3.2xlarge',\n",
    "     base_job_name=(model_name.split(\"/\")[-1]).replace(\"_\",\"-\"),   \n",
    "     instance_count=1,\n",
    "     role=role,\n",
    "    #  git_config=git_config,\n",
    "     transformers_version='4.17.0',\n",
    "     pytorch_version='1.10.2',\n",
    "     py_version='py38',\n",
    "     hyperparameters = hyperparameters,\n",
    "     output_path=output_path,\n",
    "#  If operating on ml.p3.4xlarge and above we can opt for distributed computing to reduce training time.\n",
    "#      distribution ={\"mpi\": { \"enabled\": True },\"smdistributed\": {\"modelparallel\": { \"enabled\": True,\"parameters\": {}}}},\n",
    "     metric_definitions=metric_definitions,\n",
    "    )\n",
    "    \n",
    "    huggingface_estimator.fit({\"train\": f\"s3://{Bucket_Name}/{train_test_val_location_S3}/train\",\n",
    "                           \"test\": f\"s3://{Bucket_Name}/{train_test_val_location_S3}/val\"})\n",
    "    return(huggingface_estimator)\n",
    "# starting the train job\n",
    "# huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae70e15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-03 12:24:04 Starting - Starting the training job...\n",
      "2022-07-03 12:24:30 Starting - Preparing the instances for trainingProfilerReport-1656851044: InProgress\n",
      ".........\n",
      "2022-07-03 12:26:00 Downloading - Downloading input data\n",
      "2022-07-03 12:26:00 Training - Downloading the training image.............................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\u001b[0m\n",
      "\u001b[34m2022-07-03 12:30:54,357 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-07-03 12:30:54,380 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-07-03 12:30:54,386 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-07-03 12:30:54,859 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 19.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting rouge_score\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (4.63.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (2022.4.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[34mCollecting absl-py\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.1.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.7/123.7 kB 11.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.1.0 nltk-3.7 rouge_score-0.0.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mWARNING: There was an error checking the latest version of pip.\u001b[0m\n",
      "\u001b[34m2022-07-03 12:30:59,114 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epoch\": 2,\n",
      "        \"eval-batch-size\": 2,\n",
      "        \"log-dir\": \"s3://model-deploy-poc-new/model/pegasus-xsum/Logs\",\n",
      "        \"model-name\": \"google/pegasus-xsum\",\n",
      "        \"target-column\": \"Summary\",\n",
      "        \"text-column\": \"Article\",\n",
      "        \"train-batch-size\": 5,\n",
      "        \"train-data-dir\": \"s3://model-deploy-poc-new/train_test_val\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pegasus-xsum-2022-07-03-12-24-04-505\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://model-deploy-poc-new/pegasus-xsum-2022-07-03-12-24-04-505/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epoch\":2,\"eval-batch-size\":2,\"log-dir\":\"s3://model-deploy-poc-new/model/pegasus-xsum/Logs\",\"model-name\":\"google/pegasus-xsum\",\"target-column\":\"Summary\",\"text-column\":\"Article\",\"train-batch-size\":5,\"train-data-dir\":\"s3://model-deploy-poc-new/train_test_val\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://model-deploy-poc-new/pegasus-xsum-2022-07-03-12-24-04-505/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epoch\":2,\"eval-batch-size\":2,\"log-dir\":\"s3://model-deploy-poc-new/model/pegasus-xsum/Logs\",\"model-name\":\"google/pegasus-xsum\",\"target-column\":\"Summary\",\"text-column\":\"Article\",\"train-batch-size\":5,\"train-data-dir\":\"s3://model-deploy-poc-new/train_test_val\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pegasus-xsum-2022-07-03-12-24-04-505\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://model-deploy-poc-new/pegasus-xsum-2022-07-03-12-24-04-505/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epoch\",\"2\",\"--eval-batch-size\",\"2\",\"--log-dir\",\"s3://model-deploy-poc-new/model/pegasus-xsum/Logs\",\"--model-name\",\"google/pegasus-xsum\",\"--target-column\",\"Summary\",\"--text-column\",\"Article\",\"--train-batch-size\",\"5\",\"--train-data-dir\",\"s3://model-deploy-poc-new/train_test_val\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=2\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL-BATCH-SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_LOG-DIR=s3://model-deploy-poc-new/model/pegasus-xsum/Logs\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL-NAME=google/pegasus-xsum\u001b[0m\n",
      "\u001b[34mSM_HP_TARGET-COLUMN=Summary\u001b[0m\n",
      "\u001b[34mSM_HP_TEXT-COLUMN=Article\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN-BATCH-SIZE=5\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN-DATA-DIR=s3://model-deploy-poc-new/train_test_val\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --epoch 2 --eval-batch-size 2 --log-dir s3://model-deploy-poc-new/model/pegasus-xsum/Logs --model-name google/pegasus-xsum --target-column Summary --text-column Article --train-batch-size 5 --train-data-dir s3://model-deploy-poc-new/train_test_val\u001b[0m\n",
      "\n",
      "2022-07-03 12:31:09 Training - Training image download completed. Training in progress.\u001b[34m**********************************************************************\n",
      "  Resource #033[93mpunkt#033[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "  #033[31m>>> import nltk\n",
      "  >>> nltk.download('punkt')\n",
      "  #033[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "  Attempted to load #033[93mtokenizers/punkt#033[0m\n",
      "  Searched in:\n",
      "    - '/root/nltk_data'\n",
      "    - '/opt/conda/nltk_data'\n",
      "    - '/opt/conda/share/nltk_data'\n",
      "    - '/opt/conda/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\u001b[0m\n",
      "\u001b[34m**********************************************************************\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping tokenizers/punkt.zip.\u001b[0m\n",
      "\u001b[34mLoading tokenizer...\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/87.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 87.0/87.0 [00:00<00:00, 84.1kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 1.36k/1.36k [00:00<00:00, 1.37MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/1.82M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 1.82M/1.82M [00:00<00:00, 20.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/3.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 2.28M/3.36M [00:00<00:00, 23.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 3.36M/3.36M [00:00<00:00, 29.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 65.0/65.0 [00:00<00:00, 69.7kB/s]\u001b[0m\n",
      "\u001b[34mLoading pretrained model\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/2.12G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 704k/2.12G [00:00<05:16, 7.20MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 5.02M/2.12G [00:00<01:19, 28.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 8.55M/2.12G [00:00<01:09, 32.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 12.6M/2.12G [00:00<01:02, 36.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 17.2M/2.12G [00:00<00:55, 40.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 22.5M/2.12G [00:00<00:49, 45.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 26.8M/2.12G [00:00<00:49, 45.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|▏         | 31.9M/2.12G [00:00<00:46, 47.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 37.5M/2.12G [00:00<00:43, 51.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 43.4M/2.12G [00:01<00:40, 54.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 49.4M/2.12G [00:01<00:38, 57.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 55.5M/2.12G [00:01<00:37, 59.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 61.7M/2.12G [00:01<00:36, 60.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 67.9M/2.12G [00:01<00:35, 62.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 74.0M/2.12G [00:01<00:35, 62.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▎         | 80.3M/2.12G [00:01<00:34, 63.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 86.4M/2.12G [00:01<00:34, 63.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 92.6M/2.12G [00:01<00:33, 64.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▍         | 98.8M/2.12G [00:01<00:33, 64.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▍         | 105M/2.12G [00:02<00:33, 64.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 111M/2.12G [00:02<00:33, 64.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 117M/2.12G [00:02<00:33, 65.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 124M/2.12G [00:02<00:33, 64.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 130M/2.12G [00:02<00:33, 64.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▋         | 136M/2.12G [00:02<00:33, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 142M/2.12G [00:02<00:32, 65.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 148M/2.12G [00:02<00:32, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 155M/2.12G [00:02<00:32, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 161M/2.12G [00:02<00:32, 64.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 167M/2.12G [00:03<00:32, 64.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 173M/2.12G [00:03<00:32, 64.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 179M/2.12G [00:03<00:32, 65.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▊         | 186M/2.12G [00:03<00:31, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 192M/2.12G [00:03<00:31, 65.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 198M/2.12G [00:03<00:31, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 204M/2.12G [00:03<00:31, 65.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|▉         | 211M/2.12G [00:03<00:31, 65.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|▉         | 217M/2.12G [00:03<00:31, 65.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|█         | 223M/2.12G [00:03<00:31, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 229M/2.12G [00:04<00:31, 65.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 236M/2.12G [00:04<00:31, 65.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 242M/2.12G [00:04<00:30, 65.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█▏        | 248M/2.12G [00:04<00:31, 64.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 254M/2.12G [00:04<00:30, 65.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 261M/2.12G [00:04<00:31, 63.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 267M/2.12G [00:04<00:31, 64.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 273M/2.12G [00:04<00:31, 64.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 279M/2.12G [00:04<00:30, 64.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 285M/2.12G [00:04<00:31, 61.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 291M/2.12G [00:05<00:32, 61.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▎        | 297M/2.12G [00:05<00:31, 62.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 304M/2.12G [00:05<00:31, 63.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 310M/2.12G [00:05<00:30, 63.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▍        | 316M/2.12G [00:05<00:30, 63.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▍        | 322M/2.12G [00:05<00:30, 64.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▌        | 328M/2.12G [00:05<00:29, 64.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▌        | 334M/2.12G [00:05<00:29, 64.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 341M/2.12G [00:05<00:29, 64.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 347M/2.12G [00:05<00:29, 64.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▋        | 353M/2.12G [00:06<00:29, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 359M/2.12G [00:06<00:29, 64.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 366M/2.12G [00:06<00:29, 65.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 372M/2.12G [00:06<00:29, 64.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 378M/2.12G [00:06<00:28, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 384M/2.12G [00:06<00:28, 64.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 390M/2.12G [00:06<00:28, 64.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 397M/2.12G [00:06<00:28, 64.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▊        | 403M/2.12G [00:06<00:28, 65.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 409M/2.12G [00:06<00:28, 64.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 415M/2.12G [00:07<00:28, 64.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 421M/2.12G [00:07<00:28, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|█▉        | 428M/2.12G [00:07<00:28, 63.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|█▉        | 434M/2.12G [00:07<00:28, 63.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|██        | 440M/2.12G [00:07<00:28, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 446M/2.12G [00:07<00:28, 63.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 452M/2.12G [00:07<00:28, 63.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 458M/2.12G [00:07<00:28, 63.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██▏       | 464M/2.12G [00:07<00:28, 62.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 470M/2.12G [00:07<00:29, 60.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 476M/2.12G [00:08<00:31, 57.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 481M/2.12G [00:08<00:32, 54.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 486M/2.12G [00:08<00:32, 53.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 492M/2.12G [00:08<00:31, 55.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 498M/2.12G [00:08<00:31, 55.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 503M/2.12G [00:08<00:32, 54.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 509M/2.12G [00:08<00:30, 56.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▎       | 515M/2.12G [00:08<00:29, 58.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 520M/2.12G [00:08<00:29, 59.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 527M/2.12G [00:09<00:28, 60.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▍       | 533M/2.12G [00:09<00:27, 61.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▍       | 539M/2.12G [00:09<00:27, 62.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 545M/2.12G [00:09<00:26, 63.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 551M/2.12G [00:09<00:26, 63.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 557M/2.12G [00:09<00:26, 64.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 564M/2.12G [00:09<00:26, 64.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▋       | 570M/2.12G [00:09<00:25, 65.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 576M/2.12G [00:09<00:25, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 582M/2.12G [00:09<00:25, 65.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 589M/2.12G [00:10<00:25, 65.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 595M/2.12G [00:10<00:41, 40.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 600M/2.12G [00:10<00:37, 43.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 606M/2.12G [00:10<00:34, 47.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 612M/2.12G [00:10<00:32, 50.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 618M/2.12G [00:10<00:30, 54.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▊       | 624M/2.12G [00:10<00:28, 56.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 630M/2.12G [00:10<00:27, 57.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 635M/2.12G [00:11<00:28, 57.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|██▉       | 641M/2.12G [00:11<00:27, 58.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|██▉       | 647M/2.12G [00:11<00:26, 59.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|███       | 653M/2.12G [00:11<00:26, 60.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|███       | 659M/2.12G [00:11<00:26, 60.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 665M/2.12G [00:11<00:25, 61.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 671M/2.12G [00:11<00:25, 61.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 677M/2.12G [00:11<00:25, 61.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███▏      | 683M/2.12G [00:11<00:25, 61.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 689M/2.12G [00:11<00:25, 62.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 695M/2.12G [00:12<00:26, 59.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 700M/2.12G [00:12<00:27, 56.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 706M/2.12G [00:12<00:26, 58.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 713M/2.12G [00:12<00:24, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 720M/2.12G [00:12<00:23, 64.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 727M/2.12G [00:12<00:22, 66.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 734M/2.12G [00:12<00:21, 68.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 741M/2.12G [00:12<00:21, 69.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 747M/2.12G [00:12<00:21, 68.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▍      | 754M/2.12G [00:12<00:22, 67.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▌      | 760M/2.12G [00:13<00:22, 64.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▌      | 766M/2.12G [00:13<00:23, 62.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 773M/2.12G [00:13<00:23, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 779M/2.12G [00:13<00:22, 65.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 786M/2.12G [00:13<00:21, 67.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 793M/2.12G [00:13<00:23, 61.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 799M/2.12G [00:13<00:23, 61.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 805M/2.12G [00:13<00:23, 61.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 810M/2.12G [00:13<00:23, 61.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 816M/2.12G [00:14<00:23, 61.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 822M/2.12G [00:14<00:22, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 828M/2.12G [00:14<00:22, 61.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 834M/2.12G [00:14<00:22, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▊      | 840M/2.12G [00:14<00:22, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 846M/2.12G [00:14<00:22, 62.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 852M/2.12G [00:14<00:22, 62.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 858M/2.12G [00:14<00:22, 62.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 864M/2.12G [00:14<00:21, 62.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|████      | 870M/2.12G [00:14<00:21, 62.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|████      | 876M/2.12G [00:15<00:22, 61.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 882M/2.12G [00:15<00:23, 58.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 888M/2.12G [00:15<00:22, 59.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 894M/2.12G [00:15<00:22, 60.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████▏     | 900M/2.12G [00:15<00:22, 59.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 905M/2.12G [00:15<00:22, 60.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 911M/2.12G [00:15<00:22, 58.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 917M/2.12G [00:15<00:22, 57.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 923M/2.12G [00:15<00:22, 59.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 929M/2.12G [00:15<00:21, 60.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 935M/2.12G [00:16<00:21, 59.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 940M/2.12G [00:16<00:21, 60.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▎     | 946M/2.12G [00:16<00:21, 60.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 952M/2.12G [00:16<00:21, 60.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 958M/2.12G [00:16<00:21, 60.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 964M/2.12G [00:16<00:20, 60.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 970M/2.12G [00:16<00:20, 61.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 975M/2.12G [00:16<00:20, 61.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▌     | 981M/2.12G [00:16<00:20, 61.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 987M/2.12G [00:17<00:19, 62.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 993M/2.12G [00:17<00:20, 60.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 999M/2.12G [00:17<00:20, 61.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▋     | 0.98G/2.12G [00:17<00:19, 61.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 0.99G/2.12G [00:17<00:19, 61.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 0.99G/2.12G [00:17<00:19, 62.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 1.00G/2.12G [00:17<00:19, 62.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 1.01G/2.12G [00:17<00:19, 62.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 1.01G/2.12G [00:17<00:19, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 1.02G/2.12G [00:17<00:19, 62.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 1.02G/2.12G [00:18<00:18, 62.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▊     | 1.03G/2.12G [00:18<00:18, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 1.03G/2.12G [00:18<00:18, 62.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 1.04G/2.12G [00:18<00:18, 62.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 1.05G/2.12G [00:18<00:18, 62.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 1.05G/2.12G [00:18<00:18, 62.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 1.06G/2.12G [00:18<00:18, 61.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|█████     | 1.06G/2.12G [00:18<00:19, 57.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|█████     | 1.07G/2.12G [00:18<00:19, 59.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 1.08G/2.12G [00:18<00:18, 60.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 1.08G/2.12G [00:19<00:18, 61.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████▏    | 1.09G/2.12G [00:19<00:18, 61.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 1.09G/2.12G [00:19<00:17, 61.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 1.10G/2.12G [00:19<00:17, 61.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 1.10G/2.12G [00:19<00:18, 58.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 1.11G/2.12G [00:19<00:19, 56.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 1.11G/2.12G [00:19<00:19, 54.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 1.12G/2.12G [00:19<00:19, 55.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 1.13G/2.12G [00:19<00:18, 57.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 1.13G/2.12G [00:19<00:18, 58.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▎    | 1.14G/2.12G [00:20<00:17, 59.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 1.14G/2.12G [00:20<00:17, 59.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 1.15G/2.12G [00:20<00:17, 59.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 1.15G/2.12G [00:20<00:17, 60.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▍    | 1.16G/2.12G [00:20<00:16, 60.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 1.17G/2.12G [00:20<00:16, 60.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 1.17G/2.12G [00:20<00:16, 61.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 1.18G/2.12G [00:20<00:16, 61.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 1.18G/2.12G [00:20<00:16, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 1.19G/2.12G [00:20<00:16, 62.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▋    | 1.20G/2.12G [00:21<00:15, 62.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 1.20G/2.12G [00:21<00:16, 58.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 1.21G/2.12G [00:21<00:16, 59.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 1.21G/2.12G [00:21<00:16, 60.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 1.22G/2.12G [00:21<00:15, 60.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 1.22G/2.12G [00:21<00:15, 61.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 1.23G/2.12G [00:21<00:15, 61.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 1.24G/2.12G [00:21<00:15, 62.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▊    | 1.24G/2.12G [00:21<00:15, 62.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 1.25G/2.12G [00:22<00:14, 62.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 1.25G/2.12G [00:22<00:14, 62.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 1.26G/2.12G [00:22<00:15, 58.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|█████▉    | 1.26G/2.12G [00:22<00:16, 56.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|█████▉    | 1.27G/2.12G [00:22<00:16, 54.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|██████    | 1.28G/2.12G [00:22<00:16, 53.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|██████    | 1.28G/2.12G [00:22<00:15, 56.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 1.29G/2.12G [00:22<00:16, 55.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 1.29G/2.12G [00:22<00:15, 58.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 1.30G/2.12G [00:22<00:15, 58.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 1.30G/2.12G [00:23<00:15, 57.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 1.31G/2.12G [00:23<00:15, 56.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 1.31G/2.12G [00:23<00:14, 57.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 1.32G/2.12G [00:23<00:14, 59.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 1.33G/2.12G [00:23<00:13, 61.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 1.33G/2.12G [00:23<00:13, 62.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 1.34G/2.12G [00:23<00:13, 63.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 1.34G/2.12G [00:23<00:13, 63.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▎   | 1.35G/2.12G [00:23<00:12, 64.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 1.36G/2.12G [00:23<00:12, 64.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 1.36G/2.12G [00:24<00:12, 64.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▍   | 1.37G/2.12G [00:24<00:12, 64.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▍   | 1.38G/2.12G [00:24<00:12, 63.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▌   | 1.38G/2.12G [00:24<00:12, 64.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▌   | 1.39G/2.12G [00:24<00:12, 64.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 1.39G/2.12G [00:24<00:19, 39.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 1.40G/2.12G [00:24<00:17, 43.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▋   | 1.40G/2.12G [00:25<00:16, 47.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 1.41G/2.12G [00:25<00:14, 51.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 1.42G/2.12G [00:25<00:13, 54.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 1.42G/2.12G [00:25<00:13, 56.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 1.43G/2.12G [00:25<00:12, 57.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 1.43G/2.12G [00:25<00:12, 59.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 1.44G/2.12G [00:25<00:11, 60.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 1.45G/2.12G [00:25<00:11, 62.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▊   | 1.45G/2.12G [00:25<00:11, 62.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 1.46G/2.12G [00:25<00:11, 63.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 1.46G/2.12G [00:26<00:10, 64.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 1.47G/2.12G [00:26<00:10, 64.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 1.48G/2.12G [00:26<00:11, 60.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 1.48G/2.12G [00:26<00:11, 59.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|███████   | 1.49G/2.12G [00:26<00:11, 60.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|███████   | 1.49G/2.12G [00:26<00:11, 60.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 1.50G/2.12G [00:26<00:10, 61.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 1.51G/2.12G [00:26<00:10, 62.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████▏  | 1.51G/2.12G [00:26<00:10, 63.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 1.52G/2.12G [00:26<00:10, 64.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 1.52G/2.12G [00:27<00:09, 64.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 1.53G/2.12G [00:27<00:09, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 1.54G/2.12G [00:27<00:09, 64.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 1.54G/2.12G [00:27<00:09, 64.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 1.55G/2.12G [00:27<00:09, 65.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 1.55G/2.12G [00:27<00:09, 65.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▎  | 1.56G/2.12G [00:27<00:09, 65.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 1.57G/2.12G [00:27<00:09, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 1.57G/2.12G [00:27<00:08, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 1.58G/2.12G [00:27<00:08, 65.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▍  | 1.58G/2.12G [00:28<00:08, 65.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▌  | 1.59G/2.12G [00:28<00:08, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▌  | 1.60G/2.12G [00:28<00:08, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 1.60G/2.12G [00:28<00:08, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 1.61G/2.12G [00:28<00:08, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 1.62G/2.12G [00:28<00:08, 65.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 1.62G/2.12G [00:28<00:08, 64.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 1.63G/2.12G [00:28<00:08, 64.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 1.63G/2.12G [00:28<00:08, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 1.64G/2.12G [00:28<00:07, 64.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 1.65G/2.12G [00:29<00:07, 65.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 1.65G/2.12G [00:29<00:07, 65.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 1.66G/2.12G [00:29<00:07, 65.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▊  | 1.66G/2.12G [00:29<00:07, 65.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 1.67G/2.12G [00:29<00:07, 65.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 1.68G/2.12G [00:29<00:07, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 1.68G/2.12G [00:29<00:07, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|███████▉  | 1.69G/2.12G [00:29<00:07, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|███████▉  | 1.69G/2.12G [00:29<00:07, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|████████  | 1.70G/2.12G [00:29<00:06, 64.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 1.71G/2.12G [00:30<00:06, 64.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 1.71G/2.12G [00:30<00:06, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 1.72G/2.12G [00:30<00:06, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████▏ | 1.72G/2.12G [00:30<00:06, 64.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 1.73G/2.12G [00:30<00:06, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 1.74G/2.12G [00:30<00:06, 64.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 1.74G/2.12G [00:30<00:06, 65.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 1.75G/2.12G [00:30<00:06, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 1.76G/2.12G [00:30<00:06, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 1.76G/2.12G [00:30<00:05, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 1.77G/2.12G [00:31<00:05, 65.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▎ | 1.77G/2.12G [00:31<00:05, 65.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 1.78G/2.12G [00:31<00:05, 65.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 1.79G/2.12G [00:31<00:05, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▍ | 1.79G/2.12G [00:31<00:05, 63.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▍ | 1.80G/2.12G [00:31<00:07, 46.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 1.80G/2.12G [00:38<01:50, 3.07MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 1.81G/2.12G [00:38<01:27, 3.85MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 1.81G/2.12G [00:38<01:11, 4.67MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 1.81G/2.12G [00:38<00:51, 6.35MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 1.82G/2.12G [00:38<00:37, 8.54MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 1.82G/2.12G [00:38<00:28, 11.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▋ | 1.83G/2.12G [00:38<00:21, 14.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▋ | 1.83G/2.12G [00:38<00:16, 19.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 1.84G/2.12G [00:38<00:12, 23.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 1.84G/2.12G [00:39<00:10, 28.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 1.85G/2.12G [00:39<00:09, 32.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 1.85G/2.12G [00:39<00:07, 38.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 1.86G/2.12G [00:39<00:06, 43.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 1.86G/2.12G [00:39<00:05, 47.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 1.87G/2.12G [00:39<00:05, 51.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▊ | 1.88G/2.12G [00:39<00:04, 54.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 1.88G/2.12G [00:39<00:04, 56.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 1.89G/2.12G [00:39<00:04, 58.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 1.89G/2.12G [00:39<00:04, 59.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|████████▉ | 1.90G/2.12G [00:40<00:03, 60.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|████████▉ | 1.90G/2.12G [00:40<00:03, 60.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|█████████ | 1.91G/2.12G [00:40<00:03, 61.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|█████████ | 1.92G/2.12G [00:40<00:03, 61.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 1.92G/2.12G [00:40<00:03, 61.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 1.93G/2.12G [00:40<00:03, 61.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 1.93G/2.12G [00:40<00:03, 61.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 1.94G/2.12G [00:40<00:03, 61.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 1.95G/2.12G [00:40<00:03, 61.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 1.95G/2.12G [00:40<00:02, 61.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 1.96G/2.12G [00:41<00:02, 62.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 1.96G/2.12G [00:41<00:02, 62.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 1.97G/2.12G [00:41<00:02, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 1.97G/2.12G [00:41<00:02, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 1.98G/2.12G [00:41<00:02, 61.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▎| 1.99G/2.12G [00:41<00:02, 62.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 1.99G/2.12G [00:41<00:02, 62.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 2.00G/2.12G [00:41<00:02, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 2.00G/2.12G [00:41<00:01, 62.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 2.01G/2.12G [00:41<00:01, 62.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▌| 2.01G/2.12G [00:42<00:01, 62.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▌| 2.02G/2.12G [00:42<00:01, 62.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 2.03G/2.12G [00:42<00:01, 62.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 2.03G/2.12G [00:42<00:01, 62.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 2.04G/2.12G [00:42<00:01, 62.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▋| 2.04G/2.12G [00:42<00:01, 62.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 2.05G/2.12G [00:42<00:01, 62.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 2.06G/2.12G [00:42<00:01, 62.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 2.06G/2.12G [00:42<00:00, 62.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 2.07G/2.12G [00:42<00:00, 62.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 2.07G/2.12G [00:43<00:00, 62.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 2.08G/2.12G [00:43<00:00, 60.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 2.08G/2.12G [00:43<00:00, 56.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▊| 2.09G/2.12G [00:43<00:00, 58.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 2.10G/2.12G [00:43<00:00, 60.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 2.10G/2.12G [00:43<00:00, 61.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|█████████▉| 2.11G/2.12G [00:43<00:00, 62.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|█████████▉| 2.11G/2.12G [00:43<00:00, 62.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 2.12G/2.12G [00:43<00:00, 51.8MB/s]\u001b[0m\n",
      "\u001b[34mPretrained model loaded\u001b[0m\n",
      "\u001b[34mFetching and tokenizing data for training\u001b[0m\n",
      "\u001b[34m0%|          | 0/4 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1/4 [00:00<00:01,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 2/4 [00:00<00:00,  2.37ba/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 3/4 [00:01<00:00,  2.69ba/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 4/4 [00:01<00:00,  3.15ba/s]\u001b[0m\n",
      "\u001b[34mTokenizing data for training loaded\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 17.36ba/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00,  3.72ba/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00,  3.71ba/s]\u001b[0m\n",
      "\u001b[34mDefining training arguments\u001b[0m\n",
      "\u001b[34mDefining seq2seq Trainer\u001b[0m\n",
      "\u001b[34mStarting Training\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: Summary, Article, __index_level_0__. If Summary, Article, __index_level_0__ are not expected by `PegasusForConditionalGeneration.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: Summary, Article, __index_level_0__. If Summary, Article, __index_level_0__ are not expected by `PegasusForConditionalGeneration.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 1632\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 1632\u001b[0m\n",
      "\u001b[34mNum Epochs = 2\n",
      "  Instantaneous batch size per device = 5\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 5\u001b[0m\n",
      "\u001b[34mNum Epochs = 2\n",
      "  Instantaneous batch size per device = 5\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 5\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\n",
      "  Total optimization steps = 654\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\n",
      "  Total optimization steps = 654\u001b[0m\n",
      "\u001b[34m0%|          | 0/654 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:07.850 algo-1:34 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.058 algo-1:34 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.059 algo-1:34 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.060 algo-1:34 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.060 algo-1:34 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.061 algo-1:34 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.738 algo-1:34 INFO hook.py:560] name:model.shared.weight count_params:98409472\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.738 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.738 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.738 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.738 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.738 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.738 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.738 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.738 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.739 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.739 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.739 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.739 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.739 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.739 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.739 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.739 algo-1:34 INFO hook.py:560] name:model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.739 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.739 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.739 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.739 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.739 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.740 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.741 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.742 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.743 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.744 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.745 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.746 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.747 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.748 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.749 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.750 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.12.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.751 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.13.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.752 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.14.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.753 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.encoder.layers.15.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.encoder.layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.encoder.layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.754 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.755 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.756 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.757 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.758 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.759 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.760 algo-1:34 INFO hook.py:560] name:model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.761 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.762 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.762 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.762 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.762 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.762 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.762 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.762 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.762 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.762 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.762 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.762 algo-1:34 INFO hook.py:560] name:model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.762 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.763 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.763 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.763 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.763 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.763 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.763 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.763 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.763 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.763 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.763 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.763 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.763 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.763 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.764 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.764 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.764 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.764 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.764 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.764 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.764 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.764 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.764 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.764 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.764 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.764 algo-1:34 INFO hook.py:560] name:model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.765 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.765 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.765 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.765 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.765 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.765 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.765 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.765 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.765 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.765 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.765 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.765 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.766 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.767 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.768 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.769 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.770 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.771 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.772 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.773 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.773 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.773 algo-1:34 INFO hook.py:560] name:model.decoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.773 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.773 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.773 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.773 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.773 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.773 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.773 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.773 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.773 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.774 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.775 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.775 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.775 algo-1:34 INFO hook.py:560] name:model.decoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.775 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.775 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.775 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.775 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.775 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.775 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.775 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.775 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.775 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.776 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.777 algo-1:34 INFO hook.py:560] name:model.decoder.layers.12.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.777 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.777 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.777 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.777 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.777 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.777 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.777 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.777 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.777 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.777 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.777 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.778 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.778 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.778 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.778 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.778 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.778 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.778 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.778 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.778 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.778 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.778 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.778 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.778 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.13.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.779 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.14.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.780 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.781 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.781 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.781 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.781 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.781 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.781 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.781 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.781 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.781 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.781 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.781 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.781 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.781 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.782 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.782 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.782 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.782 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.782 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.782 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.782 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.782 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.782 algo-1:34 INFO hook.py:560] name:model.decoder.layers.15.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.782 algo-1:34 INFO hook.py:560] name:model.decoder.layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.782 algo-1:34 INFO hook.py:560] name:model.decoder.layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.783 algo-1:34 INFO hook.py:562] Total Trainable Params: 568699904\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.783 algo-1:34 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-07-03 12:32:08.786 algo-1:34 INFO hook.py:485] Hook is writing from the hook with pid: 34\u001b[0m\n",
      "\u001b[34m0%|          | 1/654 [00:03<39:37,  3.64s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/654 [00:04<21:11,  1.95s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/654 [00:05<15:15,  1.41s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/654 [00:05<12:33,  1.16s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/654 [00:06<10:55,  1.01s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 6/654 [00:07<09:59,  1.08it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 7/654 [00:08<09:23,  1.15it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 8/654 [00:08<08:53,  1.21it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 9/654 [00:09<08:50,  1.22it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 10/654 [00:10<08:49,  1.22it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 11/654 [00:11<08:36,  1.25it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 12/654 [00:12<08:25,  1.27it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 13/654 [00:12<08:16,  1.29it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 14/654 [00:13<08:10,  1.30it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 15/654 [00:14<08:04,  1.32it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 16/654 [00:15<07:58,  1.33it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 17/654 [00:15<07:54,  1.34it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 18/654 [00:16<07:50,  1.35it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 19/654 [00:17<08:04,  1.31it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 20/654 [00:18<08:01,  1.32it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 21/654 [00:18<07:55,  1.33it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 22/654 [00:19<07:53,  1.33it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 23/654 [00:20<07:51,  1.34it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 24/654 [00:21<07:49,  1.34it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 25/654 [00:21<07:46,  1.35it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 26/654 [00:22<07:49,  1.34it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 27/654 [00:23<07:45,  1.35it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 28/654 [00:24<07:45,  1.35it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 29/654 [00:24<07:46,  1.34it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 30/654 [00:25<07:43,  1.35it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 31/654 [00:26<07:39,  1.35it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 32/654 [00:26<07:39,  1.35it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 33/654 [00:27<07:40,  1.35it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 34/654 [00:28<07:42,  1.34it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 35/654 [00:29<07:43,  1.34it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 36/654 [00:29<07:43,  1.33it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 37/654 [00:30<07:47,  1.32it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 38/654 [00:31<07:50,  1.31it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 39/654 [00:32<07:49,  1.31it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 40/654 [00:33<07:47,  1.31it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 41/654 [00:33<07:46,  1.32it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 42/654 [00:34<07:43,  1.32it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 43/654 [00:35<07:39,  1.33it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 44/654 [00:36<07:35,  1.34it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 45/654 [00:36<07:33,  1.34it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 46/654 [00:37<07:35,  1.33it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 47/654 [00:38<07:34,  1.34it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 48/654 [00:39<07:31,  1.34it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 49/654 [00:39<07:28,  1.35it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 50/654 [00:40<07:30,  1.34it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 51/654 [00:41<07:31,  1.34it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 52/654 [00:41<07:27,  1.35it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 53/654 [00:42<07:29,  1.34it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 54/654 [00:43<07:28,  1.34it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 55/654 [00:44<07:31,  1.33it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 56/654 [00:45<07:31,  1.32it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 57/654 [00:45<07:34,  1.31it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 58/654 [00:46<07:33,  1.31it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 59/654 [00:47<07:29,  1.32it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 60/654 [00:48<07:27,  1.33it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 61/654 [00:48<07:37,  1.30it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 62/654 [00:49<07:30,  1.31it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 63/654 [00:50<07:26,  1.32it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 64/654 [00:51<07:23,  1.33it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 65/654 [00:51<07:21,  1.33it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 66/654 [00:52<07:23,  1.33it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 67/654 [00:53<07:20,  1.33it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 68/654 [00:54<07:25,  1.31it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 69/654 [00:54<07:27,  1.31it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 70/654 [00:55<07:27,  1.30it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 71/654 [00:56<07:39,  1.27it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 72/654 [00:57<07:43,  1.26it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 73/654 [00:58<07:51,  1.23it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 74/654 [00:58<07:45,  1.25it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 75/654 [00:59<07:40,  1.26it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 76/654 [01:00<07:34,  1.27it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 77/654 [01:01<07:30,  1.28it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 78/654 [01:02<07:28,  1.28it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 79/654 [01:02<07:24,  1.29it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 80/654 [01:03<07:21,  1.30it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 81/654 [01:04<07:19,  1.30it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 82/654 [01:05<07:15,  1.31it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 83/654 [01:05<07:16,  1.31it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 84/654 [01:06<07:20,  1.29it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 85/654 [01:07<07:16,  1.30it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 86/654 [01:08<07:15,  1.30it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 87/654 [01:08<07:17,  1.30it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 88/654 [01:09<07:14,  1.30it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 89/654 [01:10<07:12,  1.31it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 90/654 [01:11<07:09,  1.31it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 91/654 [01:11<07:05,  1.32it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 92/654 [01:12<07:04,  1.33it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 93/654 [01:13<06:59,  1.34it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 94/654 [01:14<06:58,  1.34it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 95/654 [01:14<06:59,  1.33it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 96/654 [01:15<07:02,  1.32it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 97/654 [01:16<07:04,  1.31it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 98/654 [01:17<06:59,  1.32it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 99/654 [01:17<06:55,  1.34it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 100/654 [01:18<06:54,  1.34it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 101/654 [01:19<06:51,  1.34it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 102/654 [01:20<06:51,  1.34it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 103/654 [01:20<06:48,  1.35it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 104/654 [01:21<06:48,  1.35it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 105/654 [01:22<06:47,  1.35it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 106/654 [01:23<06:48,  1.34it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 107/654 [01:23<06:49,  1.33it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 108/654 [01:24<06:48,  1.34it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 109/654 [01:25<06:54,  1.31it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 110/654 [01:26<06:52,  1.32it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 111/654 [01:26<06:48,  1.33it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 112/654 [01:27<06:49,  1.32it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 113/654 [01:28<06:48,  1.32it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 114/654 [01:29<06:48,  1.32it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 115/654 [01:29<06:48,  1.32it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 116/654 [01:30<06:50,  1.31it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 117/654 [01:31<06:48,  1.32it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 118/654 [01:32<06:46,  1.32it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 119/654 [01:33<06:45,  1.32it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 120/654 [01:33<06:41,  1.33it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 121/654 [01:34<06:38,  1.34it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 122/654 [01:35<06:43,  1.32it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 123/654 [01:36<06:42,  1.32it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 124/654 [01:36<06:44,  1.31it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 125/654 [01:37<06:40,  1.32it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 126/654 [01:38<06:38,  1.32it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 127/654 [01:39<06:37,  1.33it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 128/654 [01:39<06:34,  1.33it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 129/654 [01:40<06:32,  1.34it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 130/654 [01:41<06:42,  1.30it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 131/654 [01:42<06:41,  1.30it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 132/654 [01:42<06:39,  1.31it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 133/654 [01:43<06:41,  1.30it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 134/654 [01:44<06:39,  1.30it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 135/654 [01:45<06:38,  1.30it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 136/654 [01:45<06:38,  1.30it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 137/654 [01:46<06:38,  1.30it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 138/654 [01:47<06:34,  1.31it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 139/654 [01:48<06:32,  1.31it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 140/654 [01:49<06:32,  1.31it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 141/654 [01:49<06:29,  1.32it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 142/654 [01:50<06:31,  1.31it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 143/654 [01:51<06:42,  1.27it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 144/654 [01:52<06:40,  1.27it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 145/654 [01:52<06:33,  1.29it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 146/654 [01:53<06:27,  1.31it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 147/654 [01:54<06:30,  1.30it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 148/654 [01:55<06:28,  1.30it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 149/654 [01:55<06:28,  1.30it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 150/654 [01:56<06:28,  1.30it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 151/654 [01:57<06:33,  1.28it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 152/654 [01:58<06:30,  1.29it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 153/654 [01:59<06:23,  1.31it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 154/654 [01:59<06:20,  1.31it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 155/654 [02:00<06:15,  1.33it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 156/654 [02:01<06:13,  1.34it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 157/654 [02:02<06:09,  1.34it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 158/654 [02:02<06:07,  1.35it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 159/654 [02:03<06:05,  1.35it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 160/654 [02:04<06:05,  1.35it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 161/654 [02:04<06:06,  1.35it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 162/654 [02:05<06:03,  1.35it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 163/654 [02:06<06:01,  1.36it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 164/654 [02:07<06:00,  1.36it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 165/654 [02:07<05:59,  1.36it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 166/654 [02:08<06:00,  1.35it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 167/654 [02:09<06:00,  1.35it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 168/654 [02:10<06:00,  1.35it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 169/654 [02:10<05:59,  1.35it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 170/654 [02:11<05:57,  1.35it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 171/654 [02:12<05:59,  1.34it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 172/654 [02:13<06:02,  1.33it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 173/654 [02:13<06:02,  1.33it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 174/654 [02:14<06:02,  1.32it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 175/654 [02:15<06:02,  1.32it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 176/654 [02:16<06:04,  1.31it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 177/654 [02:16<06:02,  1.32it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 178/654 [02:17<06:03,  1.31it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 179/654 [02:18<06:06,  1.30it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 180/654 [02:19<06:06,  1.29it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 181/654 [02:20<06:04,  1.30it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 182/654 [02:20<06:05,  1.29it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 183/654 [02:21<06:01,  1.30it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 184/654 [02:22<06:00,  1.30it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 185/654 [02:23<05:57,  1.31it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 186/654 [02:23<06:10,  1.26it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 187/654 [02:24<06:04,  1.28it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 188/654 [02:25<06:02,  1.28it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 189/654 [02:26<05:59,  1.29it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 190/654 [02:26<05:56,  1.30it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 191/654 [02:27<05:55,  1.30it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 192/654 [02:28<05:53,  1.31it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 193/654 [02:29<05:51,  1.31it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 194/654 [02:30<05:51,  1.31it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 195/654 [02:30<05:52,  1.30it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 196/654 [02:31<05:49,  1.31it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 197/654 [02:32<05:46,  1.32it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 198/654 [02:33<05:41,  1.33it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 199/654 [02:33<05:40,  1.34it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 200/654 [02:34<05:45,  1.32it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 201/654 [02:35<05:44,  1.32it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 202/654 [02:36<05:39,  1.33it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 203/654 [02:36<05:43,  1.31it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 204/654 [02:37<05:42,  1.31it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 205/654 [02:38<05:39,  1.32it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 206/654 [02:39<05:35,  1.33it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 207/654 [02:39<05:35,  1.33it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 208/654 [02:40<05:34,  1.33it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 209/654 [02:41<05:32,  1.34it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 210/654 [02:42<05:32,  1.34it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 211/654 [02:42<05:30,  1.34it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 212/654 [02:43<05:37,  1.31it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 213/654 [02:44<05:34,  1.32it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 214/654 [02:45<05:36,  1.31it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 215/654 [02:45<05:42,  1.28it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 216/654 [02:46<05:37,  1.30it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 217/654 [02:47<05:37,  1.30it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 218/654 [02:48<05:39,  1.28it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 219/654 [02:49<05:37,  1.29it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 220/654 [02:49<05:37,  1.29it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 221/654 [02:50<05:34,  1.29it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 222/654 [02:51<05:31,  1.30it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 223/654 [02:52<05:32,  1.30it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 224/654 [02:52<05:30,  1.30it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 225/654 [02:53<05:31,  1.29it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 226/654 [02:54<05:31,  1.29it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 227/654 [02:55<05:28,  1.30it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 228/654 [02:56<05:28,  1.30it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 229/654 [02:56<05:43,  1.24it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 230/654 [02:57<05:46,  1.22it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 231/654 [02:58<05:41,  1.24it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 232/654 [02:59<05:32,  1.27it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 233/654 [03:00<05:25,  1.29it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 234/654 [03:00<05:20,  1.31it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 235/654 [03:01<05:16,  1.32it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 236/654 [03:02<05:13,  1.33it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 237/654 [03:03<05:18,  1.31it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 238/654 [03:03<05:15,  1.32it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 239/654 [03:04<05:11,  1.33it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 240/654 [03:05<05:14,  1.32it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 241/654 [03:06<05:14,  1.31it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 242/654 [03:06<05:14,  1.31it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 243/654 [03:07<05:14,  1.31it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 244/654 [03:08<05:14,  1.30it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 245/654 [03:09<05:13,  1.30it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 246/654 [03:09<05:11,  1.31it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 247/654 [03:10<05:09,  1.32it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 248/654 [03:11<05:08,  1.31it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 249/654 [03:12<05:10,  1.31it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 250/654 [03:12<05:09,  1.31it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 251/654 [03:13<05:07,  1.31it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 252/654 [03:14<05:07,  1.31it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 253/654 [03:15<05:06,  1.31it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 254/654 [03:15<05:08,  1.30it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 255/654 [03:16<05:03,  1.32it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 256/654 [03:17<05:02,  1.32it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 257/654 [03:18<04:59,  1.32it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 258/654 [03:18<04:59,  1.32it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 259/654 [03:19<04:55,  1.34it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 260/654 [03:20<04:55,  1.33it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 261/654 [03:21<04:56,  1.33it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 262/654 [03:21<04:55,  1.33it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 263/654 [03:22<05:03,  1.29it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 264/654 [03:23<05:05,  1.28it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 265/654 [03:24<05:03,  1.28it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 266/654 [03:25<05:00,  1.29it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 267/654 [03:25<04:57,  1.30it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 268/654 [03:26<04:55,  1.31it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 269/654 [03:27<05:00,  1.28it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 270/654 [03:28<05:00,  1.28it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 271/654 [03:29<04:55,  1.30it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 272/654 [03:29<05:01,  1.27it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 273/654 [03:30<04:54,  1.29it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 274/654 [03:31<04:52,  1.30it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 275/654 [03:32<04:47,  1.32it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 276/654 [03:32<04:44,  1.33it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 277/654 [03:33<04:44,  1.33it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 278/654 [03:34<04:41,  1.34it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 279/654 [03:35<04:39,  1.34it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 280/654 [03:35<04:38,  1.34it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 281/654 [03:36<04:36,  1.35it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 282/654 [03:37<04:34,  1.36it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 283/654 [03:37<04:33,  1.36it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 284/654 [03:38<04:32,  1.36it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 285/654 [03:39<04:37,  1.33it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 286/654 [03:40<04:36,  1.33it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 287/654 [03:40<04:33,  1.34it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 288/654 [03:41<04:35,  1.33it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 289/654 [03:42<04:36,  1.32it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 290/654 [03:43<04:33,  1.33it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 291/654 [03:44<04:33,  1.33it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 292/654 [03:44<04:33,  1.32it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 293/654 [03:45<04:31,  1.33it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 294/654 [03:46<04:32,  1.32it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 295/654 [03:47<04:31,  1.32it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 296/654 [03:47<04:31,  1.32it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 297/654 [03:48<04:31,  1.31it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 298/654 [03:49<04:35,  1.29it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 299/654 [03:50<04:34,  1.29it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 300/654 [03:50<04:32,  1.30it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 301/654 [03:51<04:33,  1.29it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 302/654 [03:52<04:33,  1.29it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 303/654 [03:53<04:29,  1.30it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 304/654 [03:53<04:25,  1.32it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 305/654 [03:54<04:25,  1.31it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 306/654 [03:55<04:23,  1.32it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 307/654 [03:56<04:26,  1.30it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 308/654 [03:57<04:26,  1.30it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 309/654 [03:57<04:28,  1.29it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 310/654 [03:58<04:30,  1.27it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 311/654 [03:59<04:28,  1.28it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 312/654 [04:00<04:25,  1.29it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 313/654 [04:00<04:23,  1.30it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 314/654 [04:01<04:23,  1.29it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 315/654 [04:02<04:29,  1.26it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 316/654 [04:03<04:25,  1.27it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 317/654 [04:04<04:22,  1.29it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 318/654 [04:04<04:21,  1.29it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 319/654 [04:05<04:19,  1.29it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 320/654 [04:06<04:18,  1.29it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 321/654 [04:07<04:21,  1.27it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 322/654 [04:08<04:19,  1.28it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 323/654 [04:08<04:18,  1.28it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 324/654 [04:09<04:17,  1.28it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 325/654 [04:10<04:15,  1.29it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 326/654 [04:11<04:13,  1.30it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 327/654 [04:11<03:59,  1.37it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4222, 'learning_rate': 1.3080000000000002e-05, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m50%|█████     | 327/654 [04:11<03:59,  1.37it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: Summary, Article, __index_level_0__. If Summary, Article, __index_level_0__ are not expected by `PegasusForConditionalGeneration.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: Summary, Article, __index_level_0__. If Summary, Article, __index_level_0__ are not expected by `PegasusForConditionalGeneration.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 82\n",
      "  Batch size = 2\u001b[0m\n",
      "\u001b[34mNum examples = 82\n",
      "  Batch size = 2\u001b[0m\n",
      "\u001b[34m0%|          | 0/41 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m5%|▍         | 2/41 [00:03<01:00,  1.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 3/41 [00:06<01:23,  2.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 4/41 [00:09<01:33,  2.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 5/41 [00:12<01:37,  2.71s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 6/41 [00:15<01:40,  2.87s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 7/41 [00:18<01:39,  2.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|█▉        | 8/41 [00:21<01:38,  3.00s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 9/41 [00:24<01:36,  3.01s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 10/41 [00:27<01:34,  3.06s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 11/41 [00:31<01:32,  3.07s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 12/41 [00:34<01:28,  3.06s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 13/41 [00:37<01:26,  3.09s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 14/41 [00:40<01:24,  3.12s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 15/41 [00:43<01:23,  3.21s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 16/41 [00:46<01:19,  3.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████▏     | 17/41 [00:50<01:15,  3.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 18/41 [00:53<01:12,  3.13s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▋     | 19/41 [00:56<01:09,  3.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 20/41 [00:59<01:06,  3.17s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 21/41 [01:02<01:03,  3.17s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 22/41 [01:05<00:59,  3.13s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 23/41 [01:08<00:55,  3.10s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 24/41 [01:11<00:52,  3.09s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 25/41 [01:14<00:49,  3.08s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 26/41 [01:18<00:46,  3.09s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 27/41 [01:21<00:43,  3.10s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 28/41 [01:24<00:40,  3.08s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 29/41 [01:27<00:36,  3.07s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 30/41 [01:30<00:33,  3.08s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 31/41 [01:33<00:31,  3.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 32/41 [01:36<00:28,  3.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 33/41 [01:40<00:25,  3.22s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 34/41 [01:43<00:22,  3.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 35/41 [01:46<00:19,  3.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 36/41 [01:50<00:16,  3.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 37/41 [01:53<00:13,  3.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 38/41 [01:56<00:09,  3.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 39/41 [01:59<00:06,  3.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 40/41 [02:03<00:03,  3.29s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 41/41 [02:06<00:00,  3.30s/it]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/2.16k [00:00<?, ?B/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading: 5.60kB [00:00, 3.13MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.9845590591430664, 'eval_rouge1': 44.9991, 'eval_rouge2': 27.8111, 'eval_rougeL': 33.6101, 'eval_rougeLsum': 33.6694, 'eval_gen_len': 62.9024, 'eval_runtime': 130.9073, 'eval_samples_per_second': 0.626, 'eval_steps_per_second': 0.313, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m50%|█████     | 327/654 [06:22<03:59,  1.37it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 41/41 [02:07<00:00,  3.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-327\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-327\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-327/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-327/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-327/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-327/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-327/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-327/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-327/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-327/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m50%|█████     | 328/654 [06:27<3:43:54, 41.21s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 329/654 [06:28<2:37:30, 29.08s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 330/654 [06:28<1:51:08, 20.58s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 331/654 [06:29<1:18:54, 14.66s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 332/654 [06:30<56:19, 10.50s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 333/654 [06:31<40:33,  7.58s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 334/654 [06:32<29:32,  5.54s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 335/654 [06:32<21:48,  4.10s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 336/654 [06:33<16:24,  3.10s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 337/654 [06:34<12:39,  2.39s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 338/654 [06:35<10:01,  1.90s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 339/654 [06:35<08:13,  1.57s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 340/654 [06:36<06:54,  1.32s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 341/654 [06:37<06:01,  1.15s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 342/654 [06:38<05:25,  1.04s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 343/654 [06:38<04:56,  1.05it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 344/654 [06:39<04:36,  1.12it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 345/654 [06:40<04:22,  1.18it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 346/654 [06:41<04:11,  1.22it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 347/654 [06:41<04:04,  1.25it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 348/654 [06:42<03:58,  1.28it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 349/654 [06:43<03:54,  1.30it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 350/654 [06:44<03:53,  1.30it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 351/654 [06:44<03:54,  1.29it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 352/654 [06:45<03:53,  1.29it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 353/654 [06:46<03:49,  1.31it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 354/654 [06:47<03:55,  1.27it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 355/654 [06:48<03:52,  1.29it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 356/654 [06:48<03:51,  1.29it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 357/654 [06:49<03:49,  1.29it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 358/654 [06:50<03:49,  1.29it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 359/654 [06:51<03:54,  1.26it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 360/654 [06:52<03:53,  1.26it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 361/654 [06:52<03:49,  1.28it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 362/654 [06:53<03:46,  1.29it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 363/654 [06:54<03:44,  1.30it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 364/654 [06:55<03:45,  1.29it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 365/654 [06:55<03:49,  1.26it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 366/654 [06:56<03:49,  1.25it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 367/654 [06:57<03:49,  1.25it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 368/654 [06:58<03:51,  1.23it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 369/654 [06:59<03:46,  1.26it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 370/654 [06:59<03:44,  1.26it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 371/654 [07:00<03:42,  1.27it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 372/654 [07:01<03:40,  1.28it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 373/654 [07:02<03:38,  1.29it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 374/654 [07:02<03:36,  1.29it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 375/654 [07:03<03:36,  1.29it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 376/654 [07:04<03:36,  1.29it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 377/654 [07:05<03:35,  1.29it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 378/654 [07:06<03:34,  1.29it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 379/654 [07:06<03:32,  1.29it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 380/654 [07:07<03:31,  1.29it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 381/654 [07:08<03:28,  1.31it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 382/654 [07:09<03:27,  1.31it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 383/654 [07:09<03:25,  1.32it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 384/654 [07:10<03:29,  1.29it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 385/654 [07:11<03:27,  1.29it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 386/654 [07:12<03:26,  1.30it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 387/654 [07:13<03:26,  1.29it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 388/654 [07:13<03:24,  1.30it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 389/654 [07:14<03:27,  1.28it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 390/654 [07:15<03:25,  1.29it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 391/654 [07:16<03:22,  1.30it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 392/654 [07:16<03:22,  1.30it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 393/654 [07:17<03:19,  1.31it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 394/654 [07:18<03:18,  1.31it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 395/654 [07:19<03:16,  1.32it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 396/654 [07:19<03:16,  1.31it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 397/654 [07:20<03:24,  1.25it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 398/654 [07:21<03:21,  1.27it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 399/654 [07:22<03:21,  1.27it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 400/654 [07:23<03:18,  1.28it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 401/654 [07:23<03:16,  1.29it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 402/654 [07:24<03:15,  1.29it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 403/654 [07:25<03:14,  1.29it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 404/654 [07:26<03:13,  1.29it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 405/654 [07:26<03:12,  1.30it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 406/654 [07:27<03:10,  1.30it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 407/654 [07:28<03:08,  1.31it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 408/654 [07:29<03:05,  1.32it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 409/654 [07:29<03:03,  1.33it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 410/654 [07:30<03:02,  1.34it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 411/654 [07:31<03:03,  1.32it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 412/654 [07:32<03:01,  1.33it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 413/654 [07:32<03:02,  1.32it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 414/654 [07:33<03:01,  1.33it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 415/654 [07:34<03:00,  1.33it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 416/654 [07:35<03:01,  1.31it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 417/654 [07:36<03:01,  1.31it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 418/654 [07:36<03:01,  1.30it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 419/654 [07:37<03:00,  1.30it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 420/654 [07:38<02:59,  1.30it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 421/654 [07:39<03:00,  1.29it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 422/654 [07:39<03:00,  1.29it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 423/654 [07:40<02:58,  1.30it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 424/654 [07:41<02:59,  1.28it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 425/654 [07:42<02:57,  1.29it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 426/654 [07:42<02:56,  1.29it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 427/654 [07:43<02:52,  1.31it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 428/654 [07:44<02:50,  1.32it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 429/654 [07:45<02:49,  1.33it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 430/654 [07:45<02:47,  1.33it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 431/654 [07:46<02:46,  1.34it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 432/654 [07:47<02:46,  1.33it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 433/654 [07:48<02:44,  1.34it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 434/654 [07:48<02:45,  1.33it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 435/654 [07:49<02:43,  1.34it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 436/654 [07:50<02:44,  1.32it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 437/654 [07:51<02:44,  1.32it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 438/654 [07:51<02:42,  1.33it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 439/654 [07:52<02:41,  1.33it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 440/654 [07:53<02:46,  1.29it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 441/654 [07:54<02:43,  1.31it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 442/654 [07:55<02:43,  1.30it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 443/654 [07:55<02:41,  1.31it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 444/654 [07:56<02:40,  1.30it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 445/654 [07:57<02:38,  1.32it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 446/654 [07:58<02:40,  1.30it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 447/654 [07:58<02:39,  1.30it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 448/654 [07:59<02:36,  1.32it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 449/654 [08:00<02:35,  1.32it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 450/654 [08:01<02:34,  1.32it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 451/654 [08:01<02:33,  1.32it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 452/654 [08:02<02:33,  1.32it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 453/654 [08:03<02:32,  1.31it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 454/654 [08:04<02:31,  1.32it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 455/654 [08:04<02:31,  1.31it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 456/654 [08:05<02:31,  1.31it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 457/654 [08:06<02:31,  1.30it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 458/654 [08:07<02:31,  1.30it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 459/654 [08:08<02:29,  1.31it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 460/654 [08:08<02:27,  1.31it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 461/654 [08:09<02:25,  1.32it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 462/654 [08:10<02:24,  1.33it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 463/654 [08:10<02:22,  1.34it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 464/654 [08:11<02:22,  1.33it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 465/654 [08:12<02:21,  1.34it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 466/654 [08:13<02:19,  1.34it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 467/654 [08:14<02:20,  1.33it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 468/654 [08:14<02:19,  1.33it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 469/654 [08:15<02:18,  1.34it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 470/654 [08:16<02:19,  1.32it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 471/654 [08:17<02:19,  1.31it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 472/654 [08:17<02:19,  1.31it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 473/654 [08:18<02:19,  1.30it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 474/654 [08:19<02:17,  1.31it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 475/654 [08:20<02:16,  1.31it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 476/654 [08:20<02:16,  1.30it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 477/654 [08:21<02:15,  1.31it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 478/654 [08:22<02:13,  1.32it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 479/654 [08:23<02:11,  1.34it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 480/654 [08:23<02:10,  1.33it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 481/654 [08:24<02:09,  1.34it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 482/654 [08:25<02:07,  1.35it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 483/654 [08:26<02:11,  1.30it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 484/654 [08:26<02:09,  1.31it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 485/654 [08:27<02:07,  1.32it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 486/654 [08:28<02:07,  1.32it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 487/654 [08:29<02:06,  1.32it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 488/654 [08:29<02:06,  1.31it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 489/654 [08:30<02:06,  1.31it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 490/654 [08:31<02:05,  1.31it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 491/654 [08:32<02:05,  1.30it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 492/654 [08:33<02:04,  1.30it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 493/654 [08:33<02:03,  1.30it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 494/654 [08:34<02:02,  1.31it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 495/654 [08:35<02:00,  1.32it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 496/654 [08:36<02:00,  1.31it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 497/654 [08:36<02:00,  1.31it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 498/654 [08:37<01:58,  1.32it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 499/654 [08:38<01:56,  1.33it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 500/654 [08:39<01:56,  1.33it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 501/654 [08:39<01:54,  1.34it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 502/654 [08:40<01:52,  1.35it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 503/654 [08:41<01:52,  1.35it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 504/654 [08:42<01:51,  1.34it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 505/654 [08:42<01:52,  1.32it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 506/654 [08:43<01:52,  1.32it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 507/654 [08:44<01:52,  1.31it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 508/654 [08:45<01:52,  1.30it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 509/654 [08:45<01:52,  1.29it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 510/654 [08:46<01:51,  1.29it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 511/654 [08:47<01:50,  1.29it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 512/654 [08:48<01:49,  1.30it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 513/654 [08:49<01:48,  1.30it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 514/654 [08:49<01:47,  1.31it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 515/654 [08:50<01:46,  1.31it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 516/654 [08:51<01:46,  1.30it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 517/654 [08:52<01:44,  1.31it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 518/654 [08:52<01:43,  1.32it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 519/654 [08:53<01:44,  1.30it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 520/654 [08:54<01:42,  1.30it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 521/654 [08:55<01:42,  1.30it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 522/654 [08:55<01:40,  1.31it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 523/654 [08:56<01:40,  1.31it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 524/654 [08:57<01:40,  1.30it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 525/654 [08:58<01:42,  1.26it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 526/654 [08:59<01:46,  1.20it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 527/654 [08:59<01:42,  1.24it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 528/654 [09:00<01:40,  1.25it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 529/654 [09:01<01:38,  1.27it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 530/654 [09:02<01:36,  1.28it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 531/654 [09:03<01:34,  1.30it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 532/654 [09:03<01:34,  1.29it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 533/654 [09:04<01:33,  1.29it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 534/654 [09:05<01:32,  1.30it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 535/654 [09:06<01:31,  1.31it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 536/654 [09:06<01:30,  1.30it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 537/654 [09:07<01:29,  1.31it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 538/654 [09:08<01:28,  1.31it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 539/654 [09:09<01:27,  1.32it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 540/654 [09:09<01:27,  1.31it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 541/654 [09:10<01:26,  1.31it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 542/654 [09:11<01:25,  1.31it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 543/654 [09:12<01:24,  1.31it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 544/654 [09:12<01:23,  1.31it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 545/654 [09:13<01:23,  1.30it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 546/654 [09:14<01:22,  1.30it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 547/654 [09:15<01:22,  1.30it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 548/654 [09:16<01:21,  1.30it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 549/654 [09:16<01:20,  1.30it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 550/654 [09:17<01:20,  1.30it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 551/654 [09:18<01:19,  1.30it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 552/654 [09:19<01:18,  1.30it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 553/654 [09:19<01:18,  1.29it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 554/654 [09:20<01:17,  1.30it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 555/654 [09:21<01:16,  1.29it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 556/654 [09:22<01:15,  1.31it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 557/654 [09:22<01:13,  1.32it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 558/654 [09:23<01:12,  1.32it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 559/654 [09:24<01:11,  1.33it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 560/654 [09:25<01:10,  1.33it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 561/654 [09:25<01:10,  1.33it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 562/654 [09:26<01:09,  1.33it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 563/654 [09:27<01:09,  1.32it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 564/654 [09:28<01:08,  1.31it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 565/654 [09:28<01:07,  1.31it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 566/654 [09:29<01:07,  1.29it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 567/654 [09:30<01:06,  1.30it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 568/654 [09:31<01:06,  1.29it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 569/654 [09:32<01:07,  1.26it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 570/654 [09:32<01:05,  1.28it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 571/654 [09:33<01:04,  1.28it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 572/654 [09:34<01:03,  1.29it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 573/654 [09:35<01:02,  1.30it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 574/654 [09:36<01:01,  1.30it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 575/654 [09:36<01:00,  1.30it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 576/654 [09:37<00:59,  1.30it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 577/654 [09:38<00:58,  1.31it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 578/654 [09:39<00:58,  1.30it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 579/654 [09:39<00:58,  1.29it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 580/654 [09:40<00:56,  1.30it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 581/654 [09:41<00:55,  1.31it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 582/654 [09:42<00:54,  1.31it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 583/654 [09:42<00:54,  1.31it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 584/654 [09:43<00:53,  1.32it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 585/654 [09:44<00:52,  1.32it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 586/654 [09:45<00:51,  1.31it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 587/654 [09:45<00:51,  1.31it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 588/654 [09:46<00:50,  1.30it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 589/654 [09:47<00:50,  1.30it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 590/654 [09:48<00:49,  1.30it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 591/654 [09:49<00:48,  1.31it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 592/654 [09:49<00:47,  1.31it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 593/654 [09:50<00:46,  1.30it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 594/654 [09:51<00:45,  1.31it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 595/654 [09:52<00:45,  1.29it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 596/654 [09:52<00:44,  1.30it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 597/654 [09:53<00:43,  1.30it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 598/654 [09:54<00:42,  1.31it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 599/654 [09:55<00:41,  1.32it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 600/654 [09:55<00:40,  1.33it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 601/654 [09:56<00:40,  1.32it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 602/654 [09:57<00:39,  1.31it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 603/654 [09:58<00:39,  1.30it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 604/654 [09:58<00:38,  1.29it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 605/654 [09:59<00:37,  1.30it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 606/654 [10:00<00:36,  1.31it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 607/654 [10:01<00:36,  1.30it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 608/654 [10:02<00:35,  1.31it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 609/654 [10:02<00:33,  1.33it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 610/654 [10:03<00:33,  1.33it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 611/654 [10:04<00:33,  1.30it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 612/654 [10:05<00:32,  1.28it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 613/654 [10:05<00:32,  1.27it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 614/654 [10:06<00:31,  1.28it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 615/654 [10:07<00:29,  1.30it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 616/654 [10:08<00:28,  1.32it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 617/654 [10:08<00:27,  1.33it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 618/654 [10:09<00:26,  1.34it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 619/654 [10:10<00:26,  1.33it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 620/654 [10:11<00:25,  1.33it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 621/654 [10:11<00:24,  1.32it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 622/654 [10:12<00:24,  1.32it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 623/654 [10:13<00:23,  1.31it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 624/654 [10:14<00:22,  1.31it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 625/654 [10:15<00:22,  1.30it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 626/654 [10:15<00:21,  1.29it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 627/654 [10:16<00:20,  1.30it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 628/654 [10:17<00:19,  1.31it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 629/654 [10:18<00:19,  1.31it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 630/654 [10:18<00:18,  1.29it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 631/654 [10:19<00:17,  1.29it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 632/654 [10:20<00:17,  1.29it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 633/654 [10:21<00:16,  1.29it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 634/654 [10:21<00:15,  1.27it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 635/654 [10:22<00:14,  1.28it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 636/654 [10:23<00:13,  1.29it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 637/654 [10:24<00:13,  1.27it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 638/654 [10:25<00:12,  1.28it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 639/654 [10:25<00:11,  1.29it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 640/654 [10:26<00:10,  1.30it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 641/654 [10:27<00:10,  1.30it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 642/654 [10:28<00:09,  1.31it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 643/654 [10:28<00:08,  1.29it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 644/654 [10:29<00:07,  1.29it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 645/654 [10:30<00:06,  1.30it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 646/654 [10:31<00:06,  1.27it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 647/654 [10:32<00:05,  1.28it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 648/654 [10:32<00:04,  1.29it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 649/654 [10:33<00:03,  1.28it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 650/654 [10:34<00:03,  1.28it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 651/654 [10:35<00:02,  1.29it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 652/654 [10:35<00:01,  1.29it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 653/654 [10:36<00:00,  1.30it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 654/654 [10:37<00:00,  1.42it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.54, 'learning_rate': 0.0, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 654/654 [10:37<00:00,  1.42it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: Summary, Article, __index_level_0__. If Summary, Article, __index_level_0__ are not expected by `PegasusForConditionalGeneration.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: Summary, Article, __index_level_0__. If Summary, Article, __index_level_0__ are not expected by `PegasusForConditionalGeneration.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 82\u001b[0m\n",
      "\u001b[34mNum examples = 82\n",
      "  Batch size = 2\u001b[0m\n",
      "\u001b[34mBatch size = 2\u001b[0m\n",
      "\u001b[34m0%|          | 0/41 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m5%|▍         | 2/41 [00:03<00:59,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 3/41 [00:06<01:21,  2.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 4/41 [00:09<01:32,  2.50s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 5/41 [00:12<01:37,  2.72s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 6/41 [00:15<01:41,  2.91s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 7/41 [00:18<01:43,  3.03s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|█▉        | 8/41 [00:21<01:40,  3.04s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 9/41 [00:25<01:38,  3.09s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 10/41 [00:28<01:35,  3.07s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 11/41 [00:31<01:32,  3.09s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 12/41 [00:34<01:29,  3.10s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 13/41 [00:37<01:26,  3.10s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 14/41 [00:40<01:23,  3.09s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 15/41 [00:43<01:19,  3.08s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 16/41 [00:46<01:17,  3.08s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████▏     | 17/41 [00:49<01:14,  3.11s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 18/41 [00:52<01:10,  3.08s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▋     | 19/41 [00:55<01:07,  3.06s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 20/41 [00:59<01:05,  3.11s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 21/41 [01:02<01:02,  3.11s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 22/41 [01:05<00:58,  3.09s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 23/41 [01:08<00:55,  3.09s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 24/41 [01:11<00:52,  3.08s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 25/41 [01:14<00:49,  3.09s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 26/41 [01:17<00:47,  3.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 27/41 [01:20<00:44,  3.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 28/41 [01:24<00:40,  3.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 29/41 [01:27<00:37,  3.12s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 30/41 [01:30<00:34,  3.11s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 31/41 [01:33<00:31,  3.12s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 32/41 [01:36<00:28,  3.12s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 33/41 [01:39<00:24,  3.12s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 34/41 [01:42<00:21,  3.12s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 35/41 [01:45<00:18,  3.12s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 36/41 [01:49<00:15,  3.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 37/41 [01:52<00:12,  3.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 38/41 [01:55<00:09,  3.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 39/41 [01:58<00:06,  3.21s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 40/41 [02:02<00:03,  3.25s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 41/41 [02:05<00:00,  3.25s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.8167946338653564, 'eval_rouge1': 43.7939, 'eval_rouge2': 26.425, 'eval_rougeL': 31.958, 'eval_rougeLsum': 31.9064, 'eval_gen_len': 62.3537, 'eval_runtime': 129.6909, 'eval_samples_per_second': 0.632, 'eval_steps_per_second': 0.316, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 654/654 [12:46<00:00,  1.42it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 41/41 [02:06<00:00,  3.25s/it]#033[A\u001b[0m\n",
      "\u001b[34m#015                                               #033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-654\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-654\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-654/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-654/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-654/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-654/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-654/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-654/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-654/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-654/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mLoading best model from /opt/ml/model/checkpoint-327 (score: 27.8111).\u001b[0m\n",
      "\u001b[34mLoading best model from /opt/ml/model/checkpoint-327 (score: 27.8111).\u001b[0m\n",
      "\u001b[34m{'train_runtime': 772.5879, 'train_samples_per_second': 4.225, 'train_steps_per_second': 0.847, 'train_loss': 1.9811197450037032, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 654/654 [12:52<00:00,  1.42it/s]#015100%|██████████| 654/654 [12:52<00:00,  1.18s/it]\u001b[0m\n",
      "\u001b[34mModel trained successfully\u001b[0m\n",
      "\u001b[34mINFO:__main__:Model trained successfully\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mModel saved successfully\u001b[0m\n",
      "\u001b[34m*** Evaluate on test set***\u001b[0m\n",
      "\u001b[34mINFO:__main__:Model saved successfully\u001b[0m\n",
      "\u001b[34mINFO:__main__:*** Evaluate on test set***\u001b[0m\n",
      "\u001b[34mThe following columns in the test set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: Summary, Article, __index_level_0__. If Summary, Article, __index_level_0__ are not expected by `PegasusForConditionalGeneration.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the test set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: Summary, Article, __index_level_0__. If Summary, Article, __index_level_0__ are not expected by `PegasusForConditionalGeneration.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m***** Running Prediction *****\u001b[0m\n",
      "\u001b[34mNum examples = 326\n",
      "  Batch size = 2\u001b[0m\n",
      "\u001b[34mNum examples = 326\n",
      "  Batch size = 2\u001b[0m\n",
      "\u001b[34m0%|          | 0/163 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 2/163 [00:03<04:06,  1.53s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/163 [00:06<05:54,  2.21s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 4/163 [00:09<06:43,  2.54s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 5/163 [00:12<07:08,  2.71s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 6/163 [00:15<07:29,  2.86s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 7/163 [00:18<07:37,  2.93s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 8/163 [00:21<07:50,  3.04s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 9/163 [00:25<08:00,  3.12s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 10/163 [00:28<08:09,  3.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 11/163 [00:31<08:06,  3.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 12/163 [00:35<08:13,  3.27s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 13/163 [00:38<08:00,  3.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 14/163 [00:41<07:51,  3.16s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 15/163 [00:44<07:45,  3.15s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 16/163 [00:47<07:43,  3.16s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 17/163 [00:50<07:47,  3.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 18/163 [00:54<07:43,  3.20s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 19/163 [00:57<07:47,  3.25s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 20/163 [01:00<07:53,  3.31s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 21/163 [01:04<07:54,  3.34s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 22/163 [01:07<07:50,  3.33s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 23/163 [01:10<07:48,  3.34s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 24/163 [01:14<07:39,  3.30s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 25/163 [01:17<07:34,  3.29s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 26/163 [01:20<07:34,  3.32s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 27/163 [01:24<07:29,  3.31s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 28/163 [01:27<07:26,  3.31s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 29/163 [01:30<07:25,  3.32s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 30/163 [01:34<07:18,  3.30s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 31/163 [01:37<07:12,  3.28s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 32/163 [01:40<07:10,  3.29s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 33/163 [01:43<07:05,  3.27s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 34/163 [01:47<07:04,  3.29s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 35/163 [01:50<07:04,  3.32s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 36/163 [01:53<06:58,  3.29s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 37/163 [01:57<06:56,  3.31s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 38/163 [02:00<06:57,  3.34s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 39/163 [02:03<06:51,  3.32s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 40/163 [02:07<06:48,  3.32s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 41/163 [02:10<06:41,  3.29s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 42/163 [02:13<06:40,  3.31s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 43/163 [02:17<06:37,  3.31s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 44/163 [02:20<06:34,  3.32s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 45/163 [02:23<06:29,  3.30s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 46/163 [02:26<06:22,  3.27s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 47/163 [02:30<06:18,  3.26s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 48/163 [02:33<06:13,  3.24s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 49/163 [02:36<06:10,  3.25s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 50/163 [02:39<06:09,  3.27s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 51/163 [02:43<06:04,  3.26s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 52/163 [02:46<05:57,  3.22s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 53/163 [02:49<05:48,  3.16s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 54/163 [02:52<05:44,  3.16s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 55/163 [02:55<05:42,  3.17s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 56/163 [02:58<05:46,  3.24s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 57/163 [03:02<05:39,  3.21s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 58/163 [03:05<05:32,  3.17s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 59/163 [03:08<05:26,  3.14s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 60/163 [03:11<05:21,  3.12s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 61/163 [03:14<05:20,  3.14s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 62/163 [03:17<05:17,  3.14s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 63/163 [03:20<05:12,  3.13s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 64/163 [03:23<05:07,  3.11s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 65/163 [03:27<05:08,  3.15s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 66/163 [03:30<05:03,  3.13s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 67/163 [03:33<04:58,  3.11s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 68/163 [03:36<04:58,  3.14s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 69/163 [03:39<04:53,  3.12s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 70/163 [03:42<04:48,  3.10s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 71/163 [03:45<04:44,  3.09s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 72/163 [03:48<04:40,  3.08s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 73/163 [03:51<04:39,  3.10s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 74/163 [03:54<04:36,  3.10s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 75/163 [03:58<04:33,  3.11s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 76/163 [04:01<04:36,  3.18s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 77/163 [04:04<04:29,  3.14s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 78/163 [04:07<04:28,  3.15s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 79/163 [04:10<04:22,  3.12s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 80/163 [04:13<04:17,  3.10s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 81/163 [04:16<04:12,  3.08s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 82/163 [04:19<04:11,  3.10s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 83/163 [04:22<04:07,  3.09s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 84/163 [04:26<04:03,  3.09s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 85/163 [04:29<04:01,  3.09s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 86/163 [04:32<03:56,  3.08s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 87/163 [04:35<03:57,  3.12s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 88/163 [04:38<03:53,  3.12s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 89/163 [04:41<03:51,  3.12s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 90/163 [04:44<03:49,  3.15s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 91/163 [04:47<03:44,  3.12s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 92/163 [04:50<03:38,  3.08s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 93/163 [04:53<03:34,  3.07s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 94/163 [04:57<03:37,  3.16s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 95/163 [05:00<03:42,  3.28s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 96/163 [05:04<03:40,  3.29s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 97/163 [05:07<03:37,  3.30s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 98/163 [05:10<03:36,  3.34s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 99/163 [05:14<03:31,  3.30s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 100/163 [05:17<03:27,  3.29s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 101/163 [05:20<03:27,  3.34s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 102/163 [05:24<03:22,  3.31s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 103/163 [05:27<03:17,  3.29s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 104/163 [05:30<03:12,  3.27s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 105/163 [05:33<03:09,  3.27s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 106/163 [05:37<03:07,  3.28s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 107/163 [05:40<03:02,  3.26s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 108/163 [05:43<02:59,  3.27s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 109/163 [05:46<02:56,  3.27s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 110/163 [05:50<02:55,  3.31s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 111/163 [05:53<02:53,  3.34s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 112/163 [05:57<02:49,  3.33s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 113/163 [06:00<02:47,  3.36s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 114/163 [06:03<02:41,  3.30s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 115/163 [06:06<02:35,  3.24s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 116/163 [06:09<02:29,  3.18s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 117/163 [06:12<02:25,  3.16s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 118/163 [06:15<02:20,  3.11s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 119/163 [06:19<02:17,  3.14s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 120/163 [06:22<02:15,  3.15s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 121/163 [06:25<02:14,  3.19s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 122/163 [06:28<02:11,  3.21s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 123/163 [06:32<02:10,  3.25s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 124/163 [06:35<02:08,  3.29s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 125/163 [06:38<02:04,  3.27s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 126/163 [06:41<02:00,  3.25s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 127/163 [06:45<01:56,  3.23s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 128/163 [06:48<01:55,  3.29s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 129/163 [06:51<01:51,  3.29s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 130/163 [06:55<01:48,  3.29s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 131/163 [06:58<01:46,  3.33s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 132/163 [07:02<01:43,  3.35s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 133/163 [07:05<01:39,  3.31s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 134/163 [07:08<01:36,  3.33s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 135/163 [07:11<01:32,  3.31s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 136/163 [07:15<01:28,  3.29s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 137/163 [07:18<01:25,  3.27s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 138/163 [07:21<01:21,  3.25s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 139/163 [07:24<01:18,  3.27s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 140/163 [07:28<01:15,  3.30s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 141/163 [07:31<01:12,  3.31s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 142/163 [07:34<01:09,  3.31s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 143/163 [07:38<01:07,  3.35s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 144/163 [07:41<01:03,  3.33s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 145/163 [07:44<00:59,  3.31s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 146/163 [07:48<00:55,  3.28s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 147/163 [07:51<00:52,  3.26s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 148/163 [07:54<00:49,  3.28s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 149/163 [07:57<00:45,  3.28s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 150/163 [08:01<00:42,  3.27s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 151/163 [08:04<00:38,  3.24s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 152/163 [08:07<00:35,  3.22s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 153/163 [08:10<00:31,  3.16s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 154/163 [08:13<00:28,  3.15s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 155/163 [08:16<00:25,  3.13s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 156/163 [08:19<00:22,  3.14s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 157/163 [08:22<00:18,  3.12s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 158/163 [08:25<00:15,  3.09s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 159/163 [08:29<00:12,  3.09s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 160/163 [08:32<00:09,  3.09s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 161/163 [08:35<00:06,  3.11s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 162/163 [08:38<00:03,  3.14s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 163/163 [08:41<00:00,  3.13s/it]\u001b[0m\n",
      "\u001b[34mPredictionOutput(predictions=array([[    0, 15917,  5342, ..., 20419,   109,     1],\n",
      "       [    0,   109,   177, ...,  5139,  5807,     1],\n",
      "       [    0,   532,   311, ...,  1790,   208,     1],\n",
      "       ...,\n",
      "       [    0,  2653,   116, ...,   584,   148,     1],\n",
      "       [    0,   110, 39255, ...,   113,   109,     1],\n",
      "       [    0,   480,  1068, ...,   134,   114,     1]]), label_ids=array([[  249,   113,   109, ...,   790,   109,     1],\n",
      "       [  109,   177, 14689, ...,  1303,   112,     1],\n",
      "       [ 4218, 10093,   149, ...,  5007,   243,     1],\n",
      "       ...,\n",
      "       [  114, 19822, 13224, ...,   140, 29673,     1],\n",
      "       [ 2689,   110, 39255, ...,   137,   179,     1],\n",
      "       [  149,   633,  1148, ...,   624,   109,     1]]), metrics={'test_loss': 1.084349513053894, 'test_rouge1': 45.4667, 'test_rouge2': 29.1256, 'test_rougeL': 34.3517, 'test_rougeLsum': 34.4054, 'test_gen_len': 61.7423, 'test_runtime': 527.8791, 'test_samples_per_second': 0.618, 'test_steps_per_second': 0.309})\u001b[0m\n",
      "\u001b[34mINFO:__main__:PredictionOutput(predictions=array([[    0, 15917,  5342, ..., 20419,   109,     1],\n",
      "       [    0,   109,   177, ...,  5139,  5807,     1],\n",
      "       [    0,   532,   311, ...,  1790,   208,     1],\n",
      "       ...,\n",
      "       [    0,  2653,   116, ...,   584,   148,     1],\n",
      "       [    0,   110, 39255, ...,   113,   109,     1],\n",
      "       [    0,   480,  1068, ...,   134,   114,     1]]), label_ids=array([[  249,   113,   109, ...,   790,   109,     1],\n",
      "       [  109,   177, 14689, ...,  1303,   112,     1],\n",
      "       [ 4218, 10093,   149, ...,  5007,   243,     1],\n",
      "       ...,\n",
      "       [  114, 19822, 13224, ...,   140, 29673,     1],\n",
      "       [ 2689,   110, 39255, ...,   137,   179,     1],\n",
      "       [  149,   633,  1148, ...,   624,   109,     1]]), metrics={'test_loss': 1.084349513053894, 'test_rouge1': 45.4667, 'test_rouge2': 29.1256, 'test_rougeL': 34.3517, 'test_rougeLsum': 34.4054, 'test_gen_len': 61.7423, 'test_runtime': 527.8791, 'test_samples_per_second': 0.618, 'test_steps_per_second': 0.309})\u001b[0m\n",
      "\u001b[34mRemoving unused checkpoints to save space in container\u001b[0m\n",
      "\u001b[34mINFO:__main__:Removing unused checkpoints to save space in container\u001b[0m\n",
      "\u001b[34m100%|██████████| 163/163 [08:45<00:00,  3.22s/it]\u001b[0m\n",
      "\u001b[34m2022-07-03 12:53:53,474 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-07-03 12:54:14 Uploading - Uploading generated training model\n",
      "2022-07-03 12:59:15 Completed - Training job completed\n",
      "ProfilerReport-1656851044: IssuesFound\n",
      "Training seconds: 2002\n",
      "Billable seconds: 2002\n"
     ]
    }
   ],
   "source": [
    "model=model_invoke(model_name=model_names,train_test_val_location_S3=train_test_val_location_S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910bc84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "model_name = 'pegasus-xsum'\n",
    "model_location=f\"s3://{Bucket_Name}/final_models/\"+model_name+\".tar.gz\"\n",
    "\n",
    "model_for_deployment = HuggingFaceModel(\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"Utilities\",\n",
    "    model_data=model_location,\n",
    "    role=role,\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    py_version=\"py36\",\n",
    "    transformers_version=\"4.6.1\",\n",
    "    name=model_name.replace(r\"_\",\"-\")+\"-V2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281cee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"summarization-endpoint-5\"+model_name+\"-1\"\n",
    "\n",
    "predictor = model_for_deployment.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    endpoint_name=endpoint_name.replace(r\"_\",\"-\"),\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7285bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict({\n",
    "'inputs': \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e52f0",
   "metadata": {},
   "source": [
    "### Average first loading time 2 Seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can opt for Async inferencing for higher load\n",
    "# from sagemaker.async_inference import AsyncInferenceConfig\n",
    "# prefix=\"async-location\"\n",
    "# endpoint_name = \"summarization-endpoint-5\"+model_name+\"-async-V1\"\n",
    "# # Create an empty AsyncInferenceConfig object to use default values\n",
    "# async_config = AsyncInferenceConfig(output_path=f\"s3://{Bucket_Name}/{prefix}/output\")\n",
    "\n",
    "# # deploy model to SageMaker Inference\n",
    "# async_predictor = model_for_deployment.deploy(\n",
    "#     async_inference_config=async_config,\n",
    "#     initial_instance_count=1, # number of instances\n",
    "#     instance_type='ml.g4dn.xlarge', # instance type,\n",
    "#     serializer=sagemaker.serializers.JSONSerializer(),\n",
    "#     deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "#     endpoint_name=endpoint_name.replace(r\"_\",\"-\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd90762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async_predictor.predict_async({\n",
    "# 'inputs': \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851b0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
